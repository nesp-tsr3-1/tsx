= Threatened Species Index (TSX) Workflow User Guide
TSX Project Team <tsx@uq.edu.au>
:description: This guide describes how to install, setup and run the Threatened Species Index (TSX).
:doctype: book
:page-layout!:
:toc: left
:toclevels: 2
:sectanchors:
:sectlinks:
:sectnums:
:icons: font
:source-highlighter: coderay
:source-language: asciidoc
:experimental:
:stem:
:idprefix:
:idseparator: -
:ast: &ast;
:dagger: pass:normal[^&dagger;^]
:endash: &#8211;
:y: icon:check[role="green"]
:n: icon:times[role="red"]
:c: icon:file-text-o[role="blue"]
:table-caption!:
:example-caption!:
:figure-caption!:
:imagesdir: images
:includedir: _includes
:underscore: _
:adp: AsciiDoc Python
:adr: Asciidoctor

// Refs
:uri-home: https://tsx.org.au
:uri-github: https://github.com/nesp-tsr3-1/tsx

NOTE: If you find errors or omissions in this document, please don't hesitate to {uri-github}/issues[submit an issue].

= Introduction

== What is the TSX?

The TSX is the world’s first Threatened Species Index. It will do for Australia’s threatened species what the ASX does for Australia’s stock market. The TSX comprises a set of indices that provide reliable and robust measures of population trends across Australia’s threatened species. It will allow users to look at threatened species trends, for all of Australia and all species altogether, or for individual regions or groups, for example migratory birds. This will enable more coherent and transparent reporting of changes in biodiversity at national, state and regional levels. The index constitutes a multi-species composite index calculated from processed and quality controlled Australian threatened and near-threatened species time-series data based on the Living Planet Index (Collen et al. 2009) approach. The Living Planet Index method requires input on species population data repeatedly recorded for a species at a survey site carried out with the same monitoring method quantifying the same unit of measurement and aggregated from raw data into a yearly time series through time. This guide exemplifies how to use an automated processing workflow pipeline to streamline all processing steps required to convert raw species population data into consistent time series for the calculation of composite multi-species trends.

== How to use this guide

This guide explains how to install and setup the TSX workflow, and then walks through the process of running the workflow on a provided sample dataset. It is highly recommended to run through this guide using the sample dataset to gain familiarity with the workflow before attempting to use it to process your own dataset. Many of the sample files provided will be useful starting points which you can modify to suit your particular dataset.

== Workflow Concepts

The TSX is produced using a workflow that begins with raw observation data collated into a relational database in a standardised way. These data are processed into time series that ultimately produce the index as well as diagnostics that provide additional context. The overall structure of the workflow is illustrated below.

image::workflow-overview.png[Workflow Overview,400]

The observation data must be provided to the workflow in a specific format and is classified as either Type 1 or Type 2 data (see <<Data Classification>>).  The workflow performs much more complex processing on Type 2 data than Type 1 data, so if your data meets the Type 1 requirements then running the workflow will be quicker and easier.

== Architecture

The data import, pre-processing and filtering steps are performed using Python scripts that operate on a relational MySQL database. The last step of Living Planet Index calculation is performed using an R script, which operates on a CSV file that is produced by preceding steps of the workflow. This is illustrated below.

image::workflow-implementation.png[Workflow Architecture,600]

A web interface can be used to import data and view the generated indices, diagnostics and processed data. This is not an essential element of the workflow and is not covered in this guide.

= Installation and Setup

== System requirements

The TSX workflow can run on Windows, macOS or Linux. 8GB RAM and 4GB available storage space is recommended.

== Installation methods

The workflow was primarily designed to run on Linux and macOS, and requires several prerequisite software packages to be installed. Performing the installation directly onto a Windows is possible, but is complicated and not recommended for most users. Instead, we supply a virtual machine image that allows you to run a Linux environment under Windows which is pre-configured to run the TSX workflow.

=== Installation using a Virtual Machine (recommended)

include::include/install-virtualbox.adoc[]

That's it! You can now skip straight to <<Running the workflow>>.

=== Installation on Linux/macOS

include::include/install-linux.adoc[]

=== Installation on Windows (advanced)

include::include/install-windows.adoc[]

= Running the workflow

== Data Import

The database and workflow tools are now configured and ready for auxiliary and observation data to be imported.

=== Taxonomic List

Before observation data can be imported, a taxonomic list must first be imported which identifies all valid taxa that will be processed by the workflow. A sample taxonomic list containing Australian birds can be found in `sample-data/TaxonList.xlsx`.

The <<Taxonomic list file format>> is a useful reference if you want to build your own taxonomic lists for use in the workflow.

Import the sample taxonomic list:

----
python -m tsx.import_taxa sample-data/TaxonList.xlsx
----

If the import is successful, the command will complete without any output.

=== Import Type 1 data

Type 1 observation data may now be imported into the database. Some sample Type 1 data can be found in `sample-data/type_1_sample.csv`. Import this data, by running the following command:

----
python -m tsx.importer --type 1 -c sample-data/type_1_sample.csv
----

The `--type 1` part of the command tells the import script that you are importing Type 1 data. This is important because Type 1 data has different requirements and is stored in a separate database table compared to Type 2 data. The `-c` flag is short for “commit” and causes the imported data to be committed to the database; without this flag, the command only performs a “dry run” and does not modify the database. This feature is also present in most of the data processing scripts, and is a useful way to test whether the data/processing is valid without actually making any change to the database.

The import script will run a range of checks on the imported data, which will generate warnings and/or errors. Warnings are advisory, while errors will prevent the data from being imported until they are fixed. This helps to ensure data quality.

The Type 1 data is now imported into the `t1_survey`, `t1_sighting` and `t1_site` database tables and is ready for data processing. You may choose to skip the rest of this section, which deals with importing Type 2 data, and proceed directly to <<Data Pre-processing & Filtering>>.

=== Import Type 2 data

Type 2 data is imported in much the same way as Type 1 data. Sample Type 2 data can be found in `sample-data/type_2_sample.csv`. Import this data by running the following command:

----
python -m tsx.importer --type 2 -c sample-data/type_2_sample.csv
----

This imports data into the `t2_survey` and `t2_sighting` tables. (Note that the `t2_site` table is populated in a separate step because Type 2 data does not have sites defined in the raw observation data).

=== Import Region Polygons

During data processing, all observations are matched to Interim Biogeographic Regionalisation subregions (SubIBRA regions). The Interim Biogeographic Regionalisation for Australia (IBRA), Version 7 classify Australia’s landscapes into 89 large geographically distinct bioregions based on common climate, geology, landform, native vegetation and species information. Within these, there are 419 subregions which are more localised and homogenous geomorphological units in each bioregion. Observations outside of SubIBRA regions are suppressed from the final output.

Import the SubIBRA regions into the database:
----
python -m tsx.import_region sample-data/spatial/Regions.shp
----

NOTE: Calling this command can take up to 20–30 minutes to process depending on you computer.

== Data Pre-processing & Filtering

Now that the observation data has been imported into the database, it is ready to be processed and filtered into an aggregated form that is suitable for LPI (Living Planet Index) analysis.

The figure below illustrates the individual steps required to process Type 1 and 2 data. Each processing step is a separate Python script that needs to be run. It is possible to run all of the scripts in a single command, however it is often useful to be able to run the steps individually especially when tweaking processing parameters and inputs. Each command stores its output in the database, so all intermediate results in the processing pipeline can be inspected and analysed.

image::workflow-preprocessing.png[Flow diagram - Data Processing Overview,400]

This flow diagram shows that Type 1 data processing is much simpler than Type 2 data processing. Type 1 data only requires 4 processing steps and 3 auxiliary input files, while Type 2 data requires 7 processing steps and 7 auxiliary input files.

In the documentation below, all steps that specific to Type 2 data only are clearly marked and can be safely skipped if you only want to process Type 1 data.


=== Aggregate Type 1 data by year/month

image::workflow-preprocessing-t1-aggregation.png[Flow diagram - Type 1 data aggregation,400,float="right"]

Observation data is aggregated to monthly resolution by grouping all records with the same month, taxon, data source, site, method (search type) and units of measurement.

Each group of records is aggregated by calculating the average value, maximum value or reporting rate (proportion of records with a non-zero value) of the individual records. Which of these three aggregation methods are used for each grouping is determined by the “Processing methods” file which specifies which aggregation method should be used for each taxon/source/method/unit combination.

For further information, see <<Processing methods file format>>.

After monthly aggregation, the data is then aggregated to yearly by averaging the monthly aggregated values.

The sample processing methods file is available at `sample-data/processing_methods.csv`. Import this by running:
----
python -m tsx.import_processing_method sample-data/processing_method.csv
----

To aggregate the Type 1 data by month and year, run the following command:
----
python -m tsx.process -c t1_aggregation
----

This will aggregate the data and put the result into the `aggregated_by_year` and `aggregated_by_month` tables.

You may now choose to skip to <<Calculate spatial representativeness (Type 1 & 2 data)>> if you are not processing Type 2 data at this stage.

=== Generate taxon alpha hulls (Type 2 data only)

image::workflow-preprocessing-alpha-hull.png[Flow diagram - attribute range and ultrataxon,400,float="right"]

Type 2 data typically contains presence-only data, however the LPI analysis requires time series based on data that include absences (or true zeros indicating non-detections). To solve this problem we need to generate (pseudo-)absences for surveys where a taxon was not recorded but is known to be sometimes present at that location. In order to identify areas where the pseudo-absences should be allocated for a species, we first generate an alpha hull based on all known observations of the species. The alpha hulls are drawn from all Type 1 and 2 data, as well as an “Incidental sightings” file which contains observation data that did not meet the Type 1 or Type 2 criteria but is still useful for determining potential presences of a taxon.

After generating these alpha hulls at the species level they are trimmed down to ultrataxon polygons by intersecting with expert-curated polygons of known taxon ranges that are defined in an auxiliary “Range Polygons” input file.

There is a sample incidental sightings input file at `sample-data/incidental_sightings.csv`, which can be imported by running:
----
python -m tsx.import_incidental sample-data/incidental_sightings.csv
----

There are sample range polygons at `sample-data/spatial/species-range/`, which can be imported by running:
----
python -m tsx.import_range sample-data/spatial/species-range
----
NOTE: Ignore the warning ‘Failed to auto identify EPSG: 7’ popping up while this processing step is running.

The specifications for both the incidental sightings file and the species range polygons can be found in the Appendix.
// TODO: Link to appendicies

After importing these files, you can then run the alpha hull processing script:
----
python -m tsx.process -c alpha_hull
----

NOTE: Calling this command may take a few minutes depending on your computer.

This will perform the alpha hull calculations, intersect the result with the range polygons, and places the result into the `taxon_presence_alpha_hull` database table.

=== Aggregate Type 2 data by year/month

image::workflow-preprocessing-t2-aggregation.png[Flow diagram - aggregate type 2 data,400,float="right"]

Now that alpha hulls have been calculated for each ultrataxon that we are interested in, we can proceed with aggregating the type 2 data.

This step of the workflow makes these assumptions about type 2 data:

* Only occupancy (UnitID=1) and abundance (UnitID=2) units are supported; sightings with any other units are ignored.
* All sightings with occupancy units (UnitID=1) are presences; i.e. there are no explicit absences with occupancy units. We refer to these as "presence-only" sightings.
* All surveys are explicity associated with a site in the imported data file.

The workflow performs the following processing steps for each ultrataxon of interest:

* A list is made of all sites associated with at least one survey that falls inside the ultrataxon's alpha hull. In other words, a list is made of all sites where the ultrataxon is known to occur.
* A list is made of all surveys in these sites. Note that each survey either contains a sighting record for the ultrataxon (a presence-only or abundance count) or it does not not (a pseudo-absence). Importantly, if the ultrataxon is a _subspecies_ and a survey contains a sighting of its parent species, that sighting is considered to be sighting of the ultrataxon for the purposes of this workflow step.
* These surveys are aggregated into monthly time series by grouping together records with the same _year_, _month_, _site_, _search type_ and _source_, calculating the following values for each group:
** The number of surveys (stem:[n_s])
** The number of surveys with presence-only sightings of the current ultrataxon (stem:[n_p])
** The number of surveys with abundance sightings of the ultrataxon (stem:[n_a])
** The sum of all counts for abundance sightings of the ultrataxon (stem:[s])
** The maximum count for abundance sightings of the ultrataxon (stem:[m])
* These values are then used to calculate the following response variables of interest:
** reporting rate: stem:[(n_p + n_a)/n_s]
** mean abundance: stem:[s/(n_s - n_p)]
** max abundance: stem:[m]
* All though all 3 response variables are calculated, only the response variables specified in the processing methods file are actually kept for further processing.
* The monthly time series are then further aggregated to yearly time series by taking the average value of the response variable across all months in each year for which data exists.

Note that the mean abundance calculation includes pseudo-absences as zeroes, rather than simply being a mean of counts of sightings with abundance units.

The type 2 aggregation we have just described can be performed by running the following command
----
python -m tsx.process -c t2_aggregation
----

This will aggregate the data and put the result into the `aggregated_by_year` and `aggregated_by_month` tables, alongside any existing aggregated type 1 data.

=== Calculate spatial representativeness (Type 1 & 2 data)

image::workflow-preprocessing-spatial-rep.png[Flow diagram - Calculate spatial representativeness,400,float="right"]

Spatial representativeness is a measure of how much of the known range of a taxon is covered by a given data source. It is calculated by generating an alpha hull based on the records for each taxon/source combination, and then measuring the proportion of the known species range that is covered by that alpha hull.

This step requires the range polygons file to be imported first. If you skipped to this section from the Type 1 data aggregation step, then you will need to import this now. A set of sample range polygons can be imported by running:
----
python -m tsx.import_range sample-data/spatial/species-range
----

The spatial representativeness processing can now be run with this command:
----
python -m tsx.process -c spatial_rep
----

This will produce alpha hulls, intersect them with the taxon core range, and populate them into the `taxon_source_alpha_hull` database table. The area of the core range and the alpha hulls is also populated so that the spatial representativeness can be calculated from this.

Note that both Type 1 and Type 2 data (if imported and processed according to all the preceding steps) will be processed by this step.


=== Filter based on suitability criteria (Type 1 & 2 data)

image::workflow-preprocessing-filter.png[Flow diagram - Filter based on suitability criteria,400,float="right"]

The final step before exporting the aggregated time series is to filter out time series that do not meet certain criteria.

The time series are not actually removed from the database in this step, instead a flag called `include_in_analysis` (found in the `aggregated_by_year` table) is updated to indicate whether or not the series should be exported in the subsequent step.

The filtering criteria applied at the time of writing are:

- Time series are limited to min/max year as defined in config file (1950-2015)
- Time series based on incidental surveys are excluded
- Taxa are excluded if the most severe EBPC/IUCN/Australian classification is Least Concern, Extinct, or not listed.
- Surveys outside of any SubIBRA region are excluded
- All-zero time series are excluded
- Data sources with certain data agreement, standardisation of method and consistency of monitoring values in the metadata are excluded
- Time series with less than 4 data points are excluded

In order to calculate these filtering criteria, data source metadata must be imported (See <<Data sources file format>>).

Sample metadata can be imported by running:
----
python -m tsx.import_data_source sample-data/data_source.csv
----

The time series can then be filtered by running:
----
python -m tsx.process -c filter_time_series
----

=== Attribute regions & metadata and export data (Type 1 & 2 data)

image::workflow-preprocessing-export.png[Flow diagram - Filter based on suitability criteria,400,float="right"]

The data is now fully processed and ready for export into the “wide table” CSV format that the LPI analysis software requires.

To export the data, run:
----
python -m tsx.process export_lpi --filter
----

This will place an output file into `sample-data/export/lpi-filtered.csv`.

This file is ready for LPI analysis!

It is also possible to export an unfiltered version:
----
python -m tsx.process export_lpi
----

or a version aggregated by month instead of year:
----
python -m tsx.process export_lpi --monthly
----

=== Run it all at once

It is possible to run all the data pre-processing & filtering in a single command:
----
python -m tsx.process -c all
----

After which you must export the data again, e.g.:
----
python -m tsx.process export_lpi --filter
----

This is useful when you import some updated input files and wish to re-run all the data processing again.

== Living Planet Index Calculation

The Living Planet Index is used to generate the main final output of the TSX workflow.

The data pre-processing & filtering generates a CSV file in a format suitable for the Living Planet Index R package, https://github.com/Zoological-Society-of-London/rlpi[`rlpi`].

To open RStudio and open an example script for generating the TSX output:
----
(cd r; rstudio lpi.R)
----

After a short delay, RStudio should appear.

image::vm-rstudio.png[RStudio screenshot - start up, 600]

Press kbd:[Shift + Ctrl + S] to run the LPI. After running successfully, a plot should appear in the bottom left window (you may need to click on the 'Plots' tab):

image::vm-rstudio-plot.png[RStudio screenshot - with plot, 600]

Congratulations! You have now run the entire TSX workflow.

=== LPI calculation via command line

Alternatively, you can run the LPI calculation directly on the command line:
----
(cd r; Rscript lpi.R)
----

This will generate the result data in a file named `infile_infile_Results.txt`, but it will not display a plot.

To run the entire workflow, including the LPI calculation, run the following command:
----
python -m tsx.process -c all && python -m tsx.process export_lpi --filter && (cd r; Rscript lpi.R)
----

// == Extra diagnostic workflow outputs
// 
// TODO brief explanation of these commands
// ----
// python -m tsx.process export alpha
// 
// python -m tsx.process export pa
// 
// python -m tsx.process export ultrataxa
// 
// python -m tsx.process export grid
// ----

= Working with your own data

== Starting afresh

Up to this point we have been working with sample data in order to gain familiarity with the TSX workflow. The purpose of this section is to explain how to run the workflow with your own input data.

If you have been working through the guide with the sample data, clear out all data from the database by running these two commands:
----
mysql tsx < data/sql/create.sql
mysql tsx < data/sql/init.sql
----

Now, work again through each step in the <<Running the workflow>> section, this time adapting each command for your particular use case. For every command that involves a file from the `sample-data` directory, you will need to evaluate whether the sample-data is appropriate for your use case, and if not, edit it or supply your own file as necessary. The <<Import File Formats>> section will be useful when working with these files.

NOTE: If you get stuck, you can get in touch by {uri-github}/issues[submitting an issue here].

== Accessing files in VirtualBox

If you have installed the TSX workflow using VirtualBox, then the entire workflow is running inside a _virtual machine_. This virtual machine saves you the hassle of installing and configuring all the different components of the TSX workflow, but it does have a drawback: *you can't access the files inside the virtual machine like the same way as ordinary files on your computer*. This presents a problem when you want to provide your own files as inputs to the workflow, or edit the sample data using the usual methods.

Fortunately, however, there are a couple of ways to get around this limitation.

=== Accessing files via network sharing

The TSX virtual machine shares its files using network sharing so that you can access them in the same way that you would access files from another computer on your network. Don't worry if your computer isn't connected to an actual network, these steps should work regardless.

To access the TSX files, open *File Explorer* and go to *Network*. If you see an error message ("Network discovery is turned off…."), you'll need to turn on Network discovery to see devices on the network that are sharing files. To turn it on, select the *Network discovery is turned off* banner, then select *Turn on network discovery and file sharing*.

You may see a TSX icon appear immediately, but if not, try typing `\\TSX` into the location bar near the top of the window and pressing enter. 

You should now be able to browse and edit files in the TSX virtual machine. If not, try the following:
 - If you have only just started the virtual machine, try waiting for a few minutes before retrying the steps above.
 - Report an issue at https://github.com/nesp-tsr3-1/tsx/issues
 - Try the alternative method, <<Accessing files via SFTP>>

=== Accessing files via SFTP

An alternative to network sharing is to access the files over SFTP.

First you will need to download an SFTP program, such as WinSCP. (https://winscp.net/eng/download.php[Download WinSCP])

Start WinSCP, and in the Login Dialog, enter the following details:

 - File protocol: `SFTP`
 - Host name: `localhost`
 - Port number: `1322`
 - User name: `tsx`
 - Password: `tsx`

Then click *Login* to connect.

You should now be able to browse the TSX files. Unlike the networking sharing method, you can't edit the files directly on the virtual machine. Instead you will have to edit the files in a folder on your computer's hard drive, and download and upload files from the virtual machine as necessary.

include::include/appendix-file-formats.adoc[]

[appendix]
= Data Classification

=== Type 1 data

Type 1 data must satisfy the following requirements:

- Species are defined to the ultrataxon level (i.e. terminal taxonomic unit of species such as species or a subspecies, hereafter referred to as ‘taxa’
- The survey methods (e.g. capture-mark-recapture surveys) are clearly defined
- The unit of measurement (e.g. number of individuals, nests, traps counted) is defined
- Data is recorded to the temporal scale of at least a year
- Spatial data for have defined accuracy of pre-defined (fixed) sites where the taxon was monitored through time
- Consistent survey methods and monitoring effort are used to monitor the taxon
- Non-detections of taxa (i.e. absence or 0 counts) are recorded and identifiable within the data

=== Type 2 data

Type 2 data must satisfy the following requirements:

- Taxon is defined at least to species level
- Survey methods are clearly defined
- The unit of measurement is defined
- Consistent survey methods and monitoring effort are used to monitor the taxon through time
- Data are recorded to the temporal scale of at least a year
- Non-detections of taxa are _not_ required, i.e. presence-only data are allowed
- Spatial coordinates are available for all sighting data points
